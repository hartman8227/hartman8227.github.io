<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we&rsquo;ll get to that.
First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.
" />
<meta name="keywords" content=", Nextcloud, llm, Ollama, NixOS" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://astaluk.com/post/241111/" />


    <title>
        
            Running Ollama on NixOS as a backend for for Nextcloud Assistant :: Paul Hartman 
        
    </title>





  <link rel="stylesheet" href="/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css" integrity="sha256-JEGDzeGjjgsI&#43;CwReRGBKI&#43;arBzJYYzW9OnncQxXaLo=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Running Ollama on NixOS as a backend for for Nextcloud Assistant">
  <meta itemprop="description" content="So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we’ll get to that.
First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.">
  <meta itemprop="datePublished" content="2024-11-17T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-11-17T00:00:00+00:00">
  <meta itemprop="wordCount" content="358">
  <meta itemprop="keywords" content="Nextcloud,Llm,Ollama,NixOS">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Running Ollama on NixOS as a backend for for Nextcloud Assistant">
  <meta name="twitter:description" content="So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we’ll get to that.
First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.">



    <meta property="og:url" content="https://astaluk.com/post/241111/">
  <meta property="og:site_name" content="Paul Hartman">
  <meta property="og:title" content="Running Ollama on NixOS as a backend for for Nextcloud Assistant">
  <meta property="og:description" content="So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we’ll get to that.
First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-11-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-11-17T00:00:00+00:00">
    <meta property="article:tag" content="Nextcloud">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Ollama">
    <meta property="article:tag" content="NixOS">






    <meta property="article:published_time" content="2024-11-17 00:00:00 &#43;0000 UTC" />












    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                hello</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/post/">Blog</a></li><li><a href="/tags/">Tags</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://astaluk.com/post/241111/">Running Ollama on NixOS as a backend for for Nextcloud Assistant</a></h2>

            
            
            

            <div class="post-content">
                <p>So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we&rsquo;ll get to that.</p>
<p>First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-configuration.nix" data-lang="configuration.nix"><span style="display:flex;"><span>  services <span style="color:#960050;background-color:#1e0010">=</span> {
</span></span><span style="display:flex;"><span>    ollama <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>      enable <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>      acceleration <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cuda&#34;</span>;
</span></span><span style="display:flex;"><span>      openFirewall <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>;
</span></span><span style="display:flex;"><span>      loadModels <span style="color:#f92672">=</span> [ <span style="color:#e6db74">&#34;llama3.1:8b&#34;</span> ];
</span></span><span style="display:flex;"><span>      host <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;100.84.238.20&#34;</span>;
</span></span><span style="display:flex;"><span>    };
</span></span><span style="display:flex;"><span>  };
</span></span></code></pre></div><ul>
<li><code>enable = true;</code> is pretty straight forward. It just installs ollama.</li>
<li><code>acceleration = &quot;cuda&quot;;</code> tells the Ollama service to use my NVIDIA GPU to speed things up. It was incredibly painful to use without that acceleration.</li>
<li><code>openFirewall = true;</code> does exactly what it sounds like. It opens the firewall to enable access the LLM.</li>
<li><code>loadModels = [ &quot;llama3.1:8b&quot;];</code> tells the module what model you want to run. You can select any model from <a href="https://huggingface.co/">HuggingFace</a> assuming your system can run it. If not <code>nixos-rebuild</code> will fail.</li>
<li><code>host</code> tells the module what interface it should be listening to for requests. It defaults to localhost.</li>
<li>Additionally you can specify what port you want ollama to use which would look like <code>port = 11434;</code> default port is 11434 and I had no reason to change it.</li>
</ul>
<p>Now we do a rebuild and we should be ready to setup the Nextcloud side. Login to Nextcloud as your admin user and go to the Administration page and select &ldquo;Artifical Intelligence&rdquo; and scroll down a-ways.</p>
<p><img src="/241111_media/ollama_nextcloud.png" alt="image"></p>
<p>In the service URL box we&rsquo;ll put the Ollama host IP address and the port number and in the second box we can put a name for the service. You may need to do some tweeking to the other settings and unfortunetly Ollama does not support image generation but things should be good for text generation now.</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://astaluk.com/tags/nextcloud/">Nextcloud</a></span>
        <span class="tag"><a href="https://astaluk.com/tags/llm/">llm</a></span>
        <span class="tag"><a href="https://astaluk.com/tags/ollama/">Ollama</a></span>
        <span class="tag"><a href="https://astaluk.com/tags/nixos/">NixOS</a></span>
        
    </p>

            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js" integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc&#43;cxaJzDdCYbAW0X1G&#43;DgZYvtKFXe6MBex8jUJ2JT25mQx&#43;YjACIng=="></script>




    </body>
</html>
