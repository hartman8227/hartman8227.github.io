<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nextcloud on Paul Hartman</title>
    <link>https://astaluk.com/tags/nextcloud/</link>
    <description>Recent content in Nextcloud on Paul Hartman</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Nov 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://astaluk.com/tags/nextcloud/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running Ollama on NixOS as a backend for for Nextcloud Assistant</title>
      <link>https://astaluk.com/post/241111/</link>
      <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://astaluk.com/post/241111/</guid>
      <description>&lt;p&gt;So Nextcloud got the ability awhile back to make use of LLMs for various things, like speech to text, text summerization and image generation. Sounded like a handy addon, and it turns out I had the hardware needed to self host it, kinda. Turns out there are some limitations, but we&amp;rsquo;ll get to that.&lt;/p&gt;&#xA;&lt;p&gt;First thing I needed to do was figure out how to run the LLM and a quick search turned up a couple of different options. The most obvious one for me to try was Ollama as it is in the Nix repos. Installing it was pretty simple, just add the following to the configuration.nix.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
